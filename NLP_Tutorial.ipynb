{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f04a287-811d-42ae-9ff2-b6af4ad48b93",
   "metadata": {},
   "source": [
    "# NLP Preprocessing Techniques\r\n",
    "\r\n",
    "This notebook provides a hands-on guide for essential preprocessing steps in Natural Language Processing (NLP), inspired by the NLP course playlist on YouTube. Each section includes explanations, examples, and code snippets to help you understand the concepts and apply them in real-world projects.\r\n",
    "\r\n",
    "## Table of Contents\r\n",
    "\r\n",
    "1. [Tokenization](#tokenization)\r\n",
    "2. [Stop Words](#stop-words)\r\n",
    "3. [Stemming](#stemming)\r\n",
    "4. [Lemmatization](#lemmatization)\r\n",
    "5. [Regular Expressions](#regular-expressions)\r\n",
    "6. [Part of Speech (POS) Tagging](#pos-tagging)\r\n",
    "7. [Named Entity Recognition (NER)](#ner)\r\n",
    "8. [Spell Checking](#spell-checking)\r\n",
    "\r\n",
    "## 1. Tokenization <a name=\"tokenization\"></a>\r\n",
    "\r\n",
    "Tokenization is the process of splitting text into smaller parts, such as words or sentences. This is the first step in many NLP tasks like **sentiment analysis**.\r\n",
    "\r\n",
    "### Example:\r\n",
    "\r\n",
    "```python\r\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\r\n",
    "\r\n",
    "text = \"NLP is fascinating. It helps computers understand human language!\"\r\n",
    "sentences = sent_tokenize(text)\r\n",
    "words = word_tokenize(text)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33579657-650b-4fd3-baca-39313ad86954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['welcome', 'our', 'students', '?', 'this', 'is', 'the', 'first', 'day', 'in', 'NLP', 'course', ',', 'let', \"'s\", 'start']\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "pragraph = 'welcome our students? this is the first day in NLP course , let\\'s start'\n",
    "words = nltk.word_tokenize(pragraph)\n",
    "sent = nltk.sent_tokenize(pragraph)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c088a50-cef7-4e93-82c8-b1c3acb8837e",
   "metadata": {},
   "source": [
    "## 2. Stop Words <a name=\"stop-words\"></a>\n",
    "Stop words are common words that don't contribute much meaning to a sentence. Removing them helps speed up training and reduce noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1f4f2642-8492-41b2-8fa7-4b8890c955da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\KMR\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords \n",
    "words = word_tokenize(pragraph)\n",
    "english_stopwords = set(stopwords.words('english'))\n",
    "new_pragraph_words = [word for word in words if word not in english_stopwords]\n",
    "print(len(words))\n",
    "print(len(new_pragraph_words))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f2cfcf-bf29-4b5e-8709-d3ff3347af3a",
   "metadata": {},
   "source": [
    "## 3. Stemming <a name=\"stemming\"></a>\n",
    "Stemming reduces words to their root form. For instance, \"running\" becomes \"run\". This helps group similar words together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a6536710-ebe9-488c-bece-34a0f67b9cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'everi', 'day', 'is', 'a', 'good', 'habit', '.', 'some', 'peopl', 'run', 'for', 'fit', ',', 'while', 'other', 'run', 'for', 'fun', '.', 'run', 'can', 'be', 'challeng', ',', 'but', 'the', 'health', 'benefit', 'of', 'be', 'a', 'runner', 'are', 'undeni', '.']\n"
     ]
    }
   ],
   "source": [
    "pragraph = '''Running every day is a good habit. \n",
    "Some people run for fitness, while others run for fun. \n",
    "Running can be challenging, \n",
    "but the health benefits of being a runner are undeniable.'''\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "words_before = word_tokenize(pragraph)\n",
    "words_after = []\n",
    "for word in words_before : \n",
    "    word = stemmer.stem(word)\n",
    "    words_after.append(word)\n",
    "print(words_after)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "afd2eb60-66ec-4dd7-a5d4-d993cc077a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\KMR\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\KMR\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d47941-71a0-4cee-9302-806f03d6b2de",
   "metadata": {},
   "source": [
    "## 4. Lemmatization <a name=\"lemmatization\"></a>\n",
    "Lemmatization is similar to stemming but ensures that the reduced form is a valid word. It looks at the word's meaning and context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "259f3802-6114-4256-9f9e-fd74e7599ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original words: ['The', 'cats', 'are', 'chasing', 'the', 'mice', 'and', 'the', 'dog', 'is', 'barking', '.']\n",
      "Lemmatized words: ['The', 'cat', 'are', 'chasing', 'the', 'mouse', 'and', 'the', 'dog', 'is', 'barking', '.']\n",
      "Lemmatized words: ['the', 'cat', 'are', 'chase', 'the', 'mice', 'and', 'the', 'dog', 'is', 'bark', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\KMR\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\KMR\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "# Download necessary resources\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "# Create a WordNetLemmatizer object\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer= PorterStemmer()\n",
    "# Sample text\n",
    "text = \"The cats are chasing the mice and the dog is barking.\"\n",
    "# Tokenize the text\n",
    "words = nltk.word_tokenize(text)\n",
    "# Apply lemmatization\n",
    "lemmas = [lemmatizer.lemmatize(word) for word in words]\n",
    "stems = [stemmer.stem(word) for word in words]\n",
    "# Print original tokens and their lemmas\n",
    "print(\"Original words:\", words)\n",
    "print(\"Lemmatized words:\", lemmas)\n",
    "print(\"Lemmatized words:\", stems)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ca47c8-1a8e-49c4-a8dd-66784a2e19b1",
   "metadata": {},
   "source": [
    "## 5. Regular Expressions <a name=\"regular-expressions\"></a>\n",
    "Regular expressions help match patterns in text, such as extracting emails or phone numbers, and cleaning data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "f883ff16-1a3b-4487-8c3c-7d55d0427b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['$54.5', '$555.255']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Sample paragraph\n",
    "text = \"\"\"\n",
    "On September 22, 2024, John Doe visited the beautiful city of Paris. \n",
    "He spent $150.75 on souvenirs and $75.50 on meals. \n",
    "During his trip, he posted about his experiences on Twitter: @JohnDoe. \n",
    "He loved visiting the Eiffel Tower and took many photos. \n",
    "For more updates, check his blog at www.johndoetravels.com. \n",
    "He also plans to visit London next month!\n",
    "\"\"\"\n",
    "\n",
    "# 1. Extract Dates\n",
    "date_pattern = r\"\\b\\w+\\s\\d{1,2},\\s\\d{4}\\b\"\n",
    "dates = re.findall(date_pattern, text)\n",
    "\n",
    "# 2. Extract Monetary Amounts\n",
    "money_pattern = r\"\\$\\d+\\.\\d{2}\"\n",
    "money_amounts = re.findall(money_pattern, text)\n",
    "\n",
    "# 3. Extract Mentions\n",
    "mention_pattern = r\"@\\w+\"\n",
    "mentions = re.findall(mention_pattern, text)\n",
    "\n",
    "# 4. Extract URLs\n",
    "url_pattern = r\"https?://[^\\s]+|www\\.[^\\s]+\"\n",
    "urls = re.findall(url_pattern, text)\n",
    "\n",
    "# 5. Extract Place Names\n",
    "place_pattern = r\"\\b(Paris|Eiffel Tower|London)\\b\"\n",
    "places = re.findall(place_pattern, text)\n",
    "\n",
    "# Print results\n",
    "print(\"Dates:\", dates)\n",
    "print(\"Monetary Amounts:\", money_amounts)\n",
    "print(\"Mentions:\", mentions)\n",
    "print(\"URLs:\", urls)\n",
    "print(\"Places:\", places)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7209357f-0a00-44e2-9e44-a8fe23b6e464",
   "metadata": {},
   "source": [
    "## 6. Part of Speech (POS) Tagging <a name=\"pos-tagging\"></a>\n",
    "POS tagging labels each word in a sentence with its corresponding part of speech (e.g., noun, verb, adjective). This is useful in various NLP tasks like translation or text summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "147b55eb-df12-4cdc-bf2f-686f6d072ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[machine, football, karim]\n",
      "lets ==== PROPN\n",
      "study ==== VERB\n",
      "machine ==== NOUN\n",
      "learning ==== VERB\n",
      "before ==== ADP\n",
      "playing ==== VERB\n",
      "football ==== NOUN\n",
      "karim ==== NOUN\n",
      "! ==== PUNCT\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "senence = 'lets study machine learning before playing football karim!'\n",
    "tokens = nlp(senence)\n",
    "print(nouns)\n",
    "\n",
    "for token in tokens :\n",
    "    print(f'{token.text} ==== {token.pos_}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078574c1-c7ae-4789-bd99-319bdef1bc31",
   "metadata": {},
   "source": [
    "## 7. Named Entity Recognition (NER) <a name=\"ner\"></a>\n",
    "NER identifies key entities like people, locations, and dates within text. This helps extract structured information from unstructured text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92f5fdeb-16ad-49f2-b1b3-1b8724b2d1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ent name April 5, 2021||  describation : DATE || explantion ||Absolute or relative dates or periods\n",
      "ent name Apple Inc.||  describation : ORG || explantion ||Companies, agencies, institutions, etc.\n",
      "ent name iPhone||  describation : ORG || explantion ||Companies, agencies, institutions, etc.\n",
      "ent name San Francisco||  describation : GPE || explantion ||Countries, cities, states\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "text = \"On April 5, 2021, Apple Inc. launched a new iPhone in San Francisco.\"\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "tokens = nlp(text)\n",
    "for ent in tokens.ents:\n",
    "    print(f'ent name : {ent}||  describation : {ent.label_} || explantion: ||{spacy.explain(ent.label_)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6d8b3c-b497-4771-9fae-cb820411190e",
   "metadata": {},
   "source": [
    "## 8. Spell Checking <a name=\"spell-checking\"></a>\n",
    "Spell checking identifies and corrects spelling mistakes in text. It can be crucial in ensuring the accuracy of text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b7a4421d-c2e4-4bd3-9674-4cda5256797c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "welcome in egypt the iss the wrong txt\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "text = 'wlcome in egpt ths iss the wrng txt'\n",
    "corrected = TextBlob(text).correct()\n",
    "print(corrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa30098b-b83f-4dd5-bf7b-b8fb44741e7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
